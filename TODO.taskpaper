Scraping Gnipe:
    - scraper daemon @done(2009-02-21 15:22:46)
    - emits 'parse' events for files that need to be parsed @done(2009-02-21 15:28:22)

Emitting Queue Events:
    - parser to parse the xml @done(2009-02-21 11:17:27)
    - event class  @done(2009-02-21 11:17:41)
    - daemon process to read from gnip-parse and emit to gnip-activity @done(2009-02-22 01:24:22)

Consuming and inserting int Couchdb:
    - tweet model to hold data  @done(2009-02-22 01:24:40)
    - fetcher to go get the tweet from upstream @done(2009-02-22 01:25:27)
    - submiter to take the tweet and put it into couchdb @done(2009-02-23 21:46:11)
    - commandline program to loop over the beanstalkd queue until empty @done(2009-02-23 21:46:13)

Viewing:
    - different databases for users ?
    - views in couchdb to show rollups @done(2009-02-23 21:46:16)
    - couchapp @done(2009-02-23 21:46:18)

Other:
    - change to have another queue between the tweet fetchers and the store
    - change stages to:
        - gnip/scraper.rb -> gnip/consume.rb @done(2009-02-25 01:59:24)
        - notify -> 'consume' @done(2009-02-25 01:59:26)
        - split @done(2009-02-25 01:59:27)
        - scrape -- start here, making sure the scrape command is all good
            - have counters for the other errors that are retrieved.
        - publish
            - have a gnip publication document
            - publish to multiple locations, one for rollowing up metrics( couch
              ? )
    - is scraping twitter okay according to twitter terms of service @done(2009-02-25 01:59:31)
    - change queues to  @done(2009-02-23 23:13:48)
        - split @done(2009-02-23 23:13:49)
        - scrape @done(2009-02-23 23:13:51)
        - publish @done(2009-02-23 23:13:52)

Gnip v2.1
    - switch to that notification stream
    - start publishing


20:53:24

